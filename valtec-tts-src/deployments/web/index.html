<!DOCTYPE html>
<html lang="vi">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Valtec Vietnamese TTS - Full ONNX Pipeline</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.17.0/dist/ort.min.js"></script>
    <script src="vietnamese_g2p.js"></script>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }

        .container {
            background: rgba(255, 255, 255, 0.05);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 40px;
            max-width: 650px;
            width: 100%;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        h1 {
            text-align: center;
            background: linear-gradient(90deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 10px;
            font-size: 1.8rem;
        }

        .subtitle {
            text-align: center;
            color: #888;
            margin-bottom: 20px;
            font-size: 14px;
        }

        .form-group {
            margin-bottom: 16px;
        }

        label {
            display: block;
            color: #fff;
            margin-bottom: 6px;
            font-weight: 500;
            font-size: 14px;
        }

        textarea {
            width: 100%;
            padding: 12px;
            border-radius: 10px;
            border: 2px solid rgba(255, 255, 255, 0.1);
            background: rgba(255, 255, 255, 0.05);
            color: #fff;
            font-size: 15px;
            resize: vertical;
            min-height: 100px;
        }

        textarea:focus {
            outline: none;
            border-color: #667eea;
        }

        select {
            width: 100%;
            padding: 10px 12px;
            border-radius: 10px;
            border: 2px solid rgba(255, 255, 255, 0.1);
            background: rgba(255, 255, 255, 0.05);
            color: #fff;
            font-size: 15px;
            cursor: pointer;
        }

        select option {
            background: #1a1a2e;
            color: #fff;
        }

        .btn {
            width: 100%;
            padding: 14px;
            border: none;
            border-radius: 10px;
            background: linear-gradient(90deg, #667eea, #764ba2);
            color: #fff;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 20px rgba(102, 126, 234, 0.4);
        }

        .btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }

        .loading {
            display: none;
            text-align: center;
            color: #667eea;
            margin: 15px 0;
        }

        .loading.active {
            display: block;
        }

        .spinner {
            width: 35px;
            height: 35px;
            border: 3px solid rgba(102, 126, 234, 0.3);
            border-top-color: #667eea;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            margin: 0 auto 8px;
        }

        @keyframes spin {
            to {
                transform: rotate(360deg);
            }
        }

        .audio-container {
            margin-top: 15px;
            display: none;
        }

        .audio-container.show {
            display: block;
        }

        audio {
            width: 100%;
            margin-top: 8px;
        }

        .status {
            text-align: center;
            padding: 8px;
            border-radius: 8px;
            margin-top: 12px;
            font-size: 13px;
            display: none;
        }

        .status.success {
            display: block;
            background: rgba(34, 197, 94, 0.2);
            color: #22c55e;
        }

        .status.error {
            display: block;
            background: rgba(239, 68, 68, 0.2);
            color: #ef4444;
        }

        .status.info {
            display: block;
            background: rgba(102, 126, 234, 0.2);
            color: #667eea;
        }

        .footer {
            text-align: center;
            margin-top: 20px;
            color: #666;
            font-size: 12px;
        }

        .mode-indicator {
            padding: 6px 12px;
            border-radius: 6px;
            font-size: 11px;
            margin-bottom: 15px;
            text-align: center;
        }

        .mode-onnx {
            background: rgba(34, 197, 94, 0.2);
            color: #22c55e;
        }

        .mode-loading {
            background: rgba(251, 191, 36, 0.2);
            color: #fbbf24;
        }

        .mode-error {
            background: rgba(239, 68, 68, 0.2);
            color: #ef4444;
        }

        .progress {
            height: 4px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 2px;
            margin-top: 8px;
            overflow: hidden;
        }

        .progress-bar {
            height: 100%;
            background: linear-gradient(90deg, #667eea, #764ba2);
            width: 0%;
            transition: width 0.3s;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>üéôÔ∏è Valtec Vietnamese TTS</h1>
        <p class="subtitle">Full ONNX Pipeline - Ch·∫°y tr·ª±c ti·∫øp tr√™n browser</p>

        <div class="mode-indicator mode-loading" id="modeIndicator">‚è≥ ƒêang t·∫£i models...</div>
        <div class="progress" id="progressContainer">
            <div class="progress-bar" id="progressBar"></div>
        </div>

        <div class="form-group">
            <label for="text">üìù Nh·∫≠p vƒÉn b·∫£n b·∫•t k·ª≥</label>
            <textarea id="text"
                placeholder="Nh·∫≠p vƒÉn b·∫£n ti·∫øng Vi·ªát ·ªü ƒë√¢y...">Xin ch√†o, t√¥i l√† h·ªá th·ªëng t·ªïng h·ª£p gi·ªçng n√≥i ti·∫øng Vi·ªát.</textarea>
        </div>

        <div class="form-group">
            <label for="speaker">üé§ Voice Selection</label>
            <select id="speaker">
                <option value="0">NF - Northern Female (B·∫Øc N·ªØ)</option>
                <option value="1" selected>SF - Southern Female (Nam N·ªØ)</option>
                <option value="2">NM1 - Northern Male 1 (B·∫Øc Nam 1)</option>
                <option value="3">SM - Southern Male (Nam Nam)</option>
                <option value="4">NM2 - Northern Male 2 (B·∫Øc Nam 2)</option>
            </select>
        </div>

        <button class="btn" id="synthesize" onclick="synthesize()" disabled>
            üîä T·∫°o gi·ªçng n√≥i
        </button>

        <div class="loading" id="loading">
            <div class="spinner"></div>
            <p id="loadingText">ƒêang x·ª≠ l√Ω...</p>
        </div>

        <div class="status" id="status"></div>

        <div class="audio-container" id="audioContainer">
            <label>üéµ K·∫øt qu·∫£</label>
            <audio id="audio" controls></audio>
        </div>

        <div class="footer">Powered by <strong>Valtec AI Team</strong> | ONNX Runtime Web</div>
    </div>

    <script>
        // ONNX Sessions
        let textEncoder = null, durationPredictor = null, flow = null, decoder = null;
        let ttsConfig = null;
        let isReady = false;
        let audioContext = null;

        const ONNX_PATH = 'https://huggingface.co/valtecAI-team/valtec-tts-onnx/resolve/main/';

        async function loadModels() {
            const indicator = document.getElementById('modeIndicator');
            const progress = document.getElementById('progressBar');
            const btn = document.getElementById('synthesize');

            try {
                progress.style.width = '10%';
                indicator.innerHTML = '‚è≥ ƒêang t·∫£i text_encoder.onnx...';
                textEncoder = await ort.InferenceSession.create(ONNX_PATH + 'text_encoder.onnx', {
                    executionProviders: ['wasm']
                });
                console.log('Text encoder loaded');

                progress.style.width = '30%';
                indicator.innerHTML = '‚è≥ ƒêang t·∫£i duration_predictor.onnx...';
                durationPredictor = await ort.InferenceSession.create(ONNX_PATH + 'duration_predictor.onnx', {
                    executionProviders: ['wasm']
                });
                console.log('Duration predictor loaded');

                progress.style.width = '50%';
                indicator.innerHTML = '‚è≥ ƒêang t·∫£i flow.onnx...';
                flow = await ort.InferenceSession.create(ONNX_PATH + 'flow.onnx', {
                    executionProviders: ['wasm']
                });
                console.log('Flow loaded');

                progress.style.width = '75%';
                indicator.innerHTML = '‚è≥ ƒêang t·∫£i decoder.onnx...';
                decoder = await ort.InferenceSession.create(ONNX_PATH + 'decoder.onnx', {
                    executionProviders: ['wasm']
                });
                console.log('Decoder loaded');

                progress.style.width = '90%';
                indicator.innerHTML = '‚è≥ ƒêang t·∫£i config...';
                const resp = await fetch(ONNX_PATH + 'tts_config.json');
                ttsConfig = await resp.json();
                console.log('Config loaded');

                progress.style.width = '100%';
                indicator.className = 'mode-indicator mode-onnx';
                indicator.innerHTML = '‚úÖ T·∫•t c·∫£ models ƒë√£ s·∫µn s√†ng! (~165 MB)';
                document.getElementById('progressContainer').style.display = 'none';

                isReady = true;
                btn.disabled = false;
                showStatus('‚úÖ S·∫µn s√†ng t·∫°o gi·ªçng n√≥i t·ª´ B·∫§T K·ª≤ vƒÉn b·∫£n n√†o!', 'success');

            } catch (error) {
                console.error('Load error:', error);
                indicator.className = 'mode-indicator mode-error';
                indicator.innerHTML = '‚ùå L·ªói t·∫£i model: ' + error.message;
                showStatus('Vui l√≤ng serve qua HTTP server', 'error');
            }
        }

        function convertTextToPhonemes(text) {
            // Use the proper Vietnamese G2P converter from vietnamese_g2p.js
            const symbolToId = ttsConfig.symbol_to_id;
            const viLangId = ttsConfig.language_id_map['VI'];

            // Use the ported G2P logic
            const g2pResult = VietnameseG2P.textToPhonemes(text, symbolToId, viLangId);

            // Add blanks between phonemes (required for TTS)
            const withBlanks = VietnameseG2P.addBlanks(g2pResult, viLangId);

            console.log('G2P result:', {
                originalPhonemes: g2pResult.phonemes.length,
                withBlanks: withBlanks.phonemes.length
            });

            return withBlanks;
        }

        async function synthesize() {
            if (!isReady) return;

            const text = document.getElementById('text').value.trim();
            const speakerId = parseInt(document.getElementById('speaker').value);
            const btn = document.getElementById('synthesize');
            const loading = document.getElementById('loading');
            const status = document.getElementById('status');
            const audioContainer = document.getElementById('audioContainer');

            if (!text) {
                showStatus('‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n', 'error');
                return;
            }

            // Hide previous results and status
            status.className = 'status';
            audioContainer.classList.remove('show');

            // Show loading
            btn.disabled = true;
            loading.classList.add('active');
            setLoadingText('ƒêang chu·∫©n b·ªã...');

            // Small delay to ensure UI updates
            await new Promise(resolve => setTimeout(resolve, 50));

            try {
                // Step 1: Text to phonemes using proper G2P
                setLoadingText('B∆∞·ªõc 1/4: Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n sang phoneme...');
                const { phonemes, tones, languages } = convertTextToPhonemes(text);
                console.log('Phonemes:', phonemes.length);

                // Prepare inputs
                const seqLen = phonemes.length;
                const phoneIds = new ort.Tensor('int64', new BigInt64Array(phonemes.map(BigInt)), [1, seqLen]);
                const phoneLengths = new ort.Tensor('int64', new BigInt64Array([BigInt(seqLen)]), [1]);
                const toneIds = new ort.Tensor('int64', new BigInt64Array(tones.map(BigInt)), [1, seqLen]);
                const languageIds = new ort.Tensor('int64', new BigInt64Array(languages.map(BigInt)), [1, seqLen]);
                const bert = new ort.Tensor('float32', new Float32Array(1024 * seqLen), [1, 1024, seqLen]);
                const jaBert = new ort.Tensor('float32', new Float32Array(768 * seqLen), [1, 768, seqLen]);
                const sid = new ort.Tensor('int64', new BigInt64Array([BigInt(speakerId)]), [1]);

                // Step 2: Text encoder
                setLoadingText('B∆∞·ªõc 2/4: Encoding vƒÉn b·∫£n...');
                const encOutputs = await textEncoder.run({
                    phone_ids: phoneIds,
                    phone_lengths: phoneLengths,
                    tone_ids: toneIds,
                    language_ids: languageIds,
                    bert: bert,
                    ja_bert: jaBert,
                    speaker_id: sid
                });

                const xEncoded = encOutputs.x_encoded;
                const mP = encOutputs.m_p;
                const logsP = encOutputs.logs_p;
                const xMask = encOutputs.x_mask;
                const g = encOutputs.g;

                console.log('Encoder output shapes:', xEncoded.dims, g.dims);

                // Step 3: Duration prediction
                setLoadingText('B∆∞·ªõc 3/4: D·ª± ƒëo√°n th·ªùi l∆∞·ª£ng...');
                const dpOutputs = await durationPredictor.run({
                    x: xEncoded,
                    x_mask: xMask,
                    g: g
                });

                const logw = dpOutputs.logw;

                // Compute durations and expand
                const logwData = logw.data;
                const maskData = xMask.data;
                let totalFrames = 0;
                const durations = [];

                for (let i = 0; i < logwData.length; i++) {
                    const dur = Math.ceil(Math.exp(logwData[i]) * maskData[i]);
                    durations.push(dur);
                    totalFrames += dur;
                }

                console.log('Total frames:', totalFrames);

                // Expand m_p and logs_p according to durations
                const mPData = mP.data;
                const logsPData = logsP.data;
                const channels = mP.dims[1];

                const expandedMP = new Float32Array(channels * totalFrames);
                const expandedLogsP = new Float32Array(channels * totalFrames);

                let frameIdx = 0;
                for (let t = 0; t < durations.length; t++) {
                    for (let d = 0; d < durations[t]; d++) {
                        for (let c = 0; c < channels; c++) {
                            expandedMP[c * totalFrames + frameIdx] = mPData[c * seqLen + t];
                            expandedLogsP[c * totalFrames + frameIdx] = logsPData[c * seqLen + t];
                        }
                        frameIdx++;
                    }
                }

                // Sample z_p
                const zP = new Float32Array(channels * totalFrames);
                for (let i = 0; i < zP.length; i++) {
                    const noise = (Math.random() * 2 - 1) * 0.667;
                    zP[i] = expandedMP[i] + Math.exp(expandedLogsP[i]) * noise;
                }

                const zPTensor = new ort.Tensor('float32', zP, [1, channels, totalFrames]);
                const yMask = new ort.Tensor('float32', new Float32Array(totalFrames).fill(1), [1, 1, totalFrames]);

                // Step 4: Flow reverse
                setLoadingText('B∆∞·ªõc 4/4: Sinh audio...');
                const flowOutputs = await flow.run({
                    z_p: zPTensor,
                    y_mask: yMask,
                    g: g
                });

                const z = flowOutputs.z;

                // Apply mask
                const zData = z.data;
                const zMasked = new Float32Array(zData.length);
                for (let i = 0; i < zData.length; i++) {
                    zMasked[i] = zData[i]; // mask is all 1s
                }

                const zTensor = new ort.Tensor('float32', zMasked, z.dims);

                // Decode to audio
                const decOutputs = await decoder.run({
                    z: zTensor,
                    g: g
                });

                const audio = decOutputs.audio;
                console.log('Audio generated:', audio.dims);

                // Play audio
                playAudio(audio.data);

                const duration = audio.data.length / ttsConfig.sample_rate;
                showStatus(`‚úÖ ƒê√£ t·∫°o ${audio.data.length.toLocaleString()} samples (${duration.toFixed(2)}s)`, 'success');

            } catch (error) {
                console.error('Synthesis error:', error);
                showStatus('‚ùå L·ªói: ' + error.message, 'error');
            } finally {
                btn.disabled = false;
                loading.classList.remove('active');
            }
        }

        function setLoadingText(text) {
            document.getElementById('loadingText').textContent = text;
        }

        async function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: ttsConfig.sample_rate
                });
            }
            if (audioContext.state === 'suspended') await audioContext.resume();
        }

        function playAudio(audioData) {
            initAudioContext();

            // Create WAV blob
            const wavBlob = audioToWav(audioData, ttsConfig.sample_rate);
            const audioUrl = URL.createObjectURL(wavBlob);

            const audio = document.getElementById('audio');
            audio.src = audioUrl;
            document.getElementById('audioContainer').classList.add('show');
            audio.play();
        }

        function audioToWav(samples, sampleRate) {
            const buffer = new ArrayBuffer(44 + samples.length * 2);
            const view = new DataView(buffer);

            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + samples.length * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(view, 36, 'data');
            view.setUint32(40, samples.length * 2, true);

            let offset = 44;
            for (let i = 0; i < samples.length; i++) {
                const s = Math.max(-1, Math.min(1, samples[i]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
                offset += 2;
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        function showStatus(message, type) {
            const status = document.getElementById('status');
            status.textContent = message;
            status.className = `status ${type}`;
        }

        // Initialize
        document.addEventListener('DOMContentLoaded', loadModels);
    </script>
</body>

</html>